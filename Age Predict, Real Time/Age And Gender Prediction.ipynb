{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is imported from the following project: https://github.com/asmith26/wide_resnets_keras\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, add, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "sys.setrecursionlimit(2 ** 20)\n",
    "np.random.seed(2 ** 10)\n",
    "\n",
    "\n",
    "class WideResNet:\n",
    "    def __init__(self, image_size, depth=16, k=8):\n",
    "        self._depth = depth\n",
    "        self._k = k\n",
    "        self._dropout_probability = 0\n",
    "        self._weight_decay = 0.0005\n",
    "        self._use_bias = False\n",
    "        self._weight_init = \"he_normal\"\n",
    "\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            logging.debug(\"image_dim_ordering = 'th'\")\n",
    "            self._channel_axis = 1\n",
    "            self._input_shape = (3, image_size, image_size)\n",
    "        else:\n",
    "            logging.debug(\"image_dim_ordering = 'tf'\")\n",
    "            self._channel_axis = -1\n",
    "            self._input_shape = (image_size, image_size, 3)\n",
    "\n",
    "    # Wide residual network http://arxiv.org/abs/1605.07146\n",
    "    def _wide_basic(self, n_input_plane, n_output_plane, stride):\n",
    "        def f(net):\n",
    "            # format of conv_params:\n",
    "            #               [ [kernel_size=(\"kernel width\", \"kernel height\"),\n",
    "            #               strides=\"(stride_vertical,stride_horizontal)\",\n",
    "            #               padding=\"same\" or \"valid\"] ]\n",
    "            # B(3,3): orignal <<basic>> block\n",
    "            conv_params = [[3, 3, stride, \"same\"],\n",
    "                           [3, 3, (1, 1), \"same\"]]\n",
    "\n",
    "            n_bottleneck_plane = n_output_plane\n",
    "\n",
    "            # Residual block\n",
    "            for i, v in enumerate(conv_params):\n",
    "                if i == 0:\n",
    "                    if n_input_plane != n_output_plane:\n",
    "                        net = BatchNormalization(axis=self._channel_axis)(net)\n",
    "                        net = Activation(\"relu\")(net)\n",
    "                        convs = net\n",
    "                    else:\n",
    "                        convs = BatchNormalization(axis=self._channel_axis)(net)\n",
    "                        convs = Activation(\"relu\")(convs)\n",
    "\n",
    "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
    "                                          strides=v[2],\n",
    "                                          padding=v[3],\n",
    "                                          kernel_initializer=self._weight_init,\n",
    "                                          kernel_regularizer=l2(self._weight_decay),\n",
    "                                          use_bias=self._use_bias)(convs)\n",
    "                else:\n",
    "                    convs = BatchNormalization(axis=self._channel_axis)(convs)\n",
    "                    convs = Activation(\"relu\")(convs)\n",
    "                    if self._dropout_probability > 0:\n",
    "                        convs = Dropout(self._dropout_probability)(convs)\n",
    "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
    "                                          strides=v[2],\n",
    "                                          padding=v[3],\n",
    "                                          kernel_initializer=self._weight_init,\n",
    "                                          kernel_regularizer=l2(self._weight_decay),\n",
    "                                          use_bias=self._use_bias)(convs)\n",
    "\n",
    "            # Shortcut Connection: identity function or 1x1 convolutional\n",
    "            #  (depends on difference between input & output shape - this\n",
    "            #   corresponds to whether we are using the first block in each\n",
    "            #   group; see _layer() ).\n",
    "            if n_input_plane != n_output_plane:\n",
    "                shortcut = Conv2D(n_output_plane, kernel_size=(1, 1),\n",
    "                                         strides=stride,\n",
    "                                         padding=\"same\",\n",
    "                                         kernel_initializer=self._weight_init,\n",
    "                                         kernel_regularizer=l2(self._weight_decay),\n",
    "                                         use_bias=self._use_bias)(net)\n",
    "            else:\n",
    "                shortcut = net\n",
    "\n",
    "            return add([convs, shortcut])\n",
    "\n",
    "        return f\n",
    "\n",
    "\n",
    "    # \"Stacking Residual Units on the same stage\"\n",
    "    def _layer(self, block, n_input_plane, n_output_plane, count, stride):\n",
    "        def f(net):\n",
    "            net = block(n_input_plane, n_output_plane, stride)(net)\n",
    "            for i in range(2, int(count + 1)):\n",
    "                net = block(n_output_plane, n_output_plane, stride=(1, 1))(net)\n",
    "            return net\n",
    "\n",
    "        return f\n",
    "\n",
    "#    def create_model(self):\n",
    "    def __call__(self):\n",
    "        logging.debug(\"Creating model...\")\n",
    "\n",
    "        assert ((self._depth - 4) % 6 == 0)\n",
    "        n = (self._depth - 4) / 6\n",
    "\n",
    "        inputs = Input(shape=self._input_shape)\n",
    "\n",
    "        n_stages = [16, 16 * self._k, 32 * self._k, 64 * self._k]\n",
    "\n",
    "        conv1 = Conv2D(filters=n_stages[0], kernel_size=(3, 3),\n",
    "                              strides=(1, 1),\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=self._weight_init,\n",
    "                              kernel_regularizer=l2(self._weight_decay),\n",
    "                              use_bias=self._use_bias)(inputs)  # \"One conv at the beginning (spatial size: 32x32)\"\n",
    "\n",
    "        # Add wide residual blocks\n",
    "        block_fn = self._wide_basic\n",
    "        conv2 = self._layer(block_fn, n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1, 1))(conv1)\n",
    "        conv3 = self._layer(block_fn, n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2, 2))(conv2)\n",
    "        conv4 = self._layer(block_fn, n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2, 2))(conv3)\n",
    "        batch_norm = BatchNormalization(axis=self._channel_axis)(conv4)\n",
    "        relu = Activation(\"relu\")(batch_norm)\n",
    "\n",
    "        # Classifier block\n",
    "        pool = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\")(relu)\n",
    "        flatten = Flatten()(pool)\n",
    "        predictions_g = Dense(units=2, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
    "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\")(flatten)\n",
    "        predictions_a = Dense(units=101, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
    "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\")(flatten)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[predictions_g, predictions_a])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Object(object):\n",
    "    \n",
    "    # This pre-trained has been taken from https://github.com/Tony607/Keras_age_gender/releases/download/V1.0/weights.18-4.06.hdf5\n",
    "    haar_cascade_classifier = \"haarcascade_frontalface_alt.xml\"\n",
    "    WRN_WEIGHTS_PATH = \".\\\\pretrained_models\\\\weights.18-4.06.hdf5\"\n",
    "    \n",
    "    def __new__(Cls, weights=None, depth=16, width=8, face_size=64):\n",
    "        if not hasattr(Cls, 'instance'):\n",
    "            Cls.instance = super(Face_Object, Cls).__new__(Cls)\n",
    "        return Cls.instance\n",
    "\n",
    "    def __init__(self, depth=16, width=8, face_size=64):\n",
    "        self.face_size = face_size\n",
    "        self.model = WideResNet(face_size, depth=depth, k=width)()\n",
    "        model_dir = os.path.join(os.getcwd(), \"pretrained_models\").replace(\"//\", \"\\\\\")\n",
    "        fpath = get_file('weights.18-4.06.hdf5',\n",
    "                        self.WRN_WEIGHTS_PATH,\n",
    "                         cache_subdir=model_dir)\n",
    "        self.model.load_weights(fpath)\n",
    "\n",
    "    def draw_label (Cls, image, point, label, font=cv2.FONT_HERSHEY_COMPLEX , font_scale=1, thickness=1):\n",
    "        size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "        x, y = point\n",
    "        cv2.rectangle (image, (x, y - size[1]), (x + size[0], y), (0,255,4), cv2.FILLED)\n",
    "        cv2.putText (image, label, point, font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "    def crop_face (self, imgarray, section, margin=40, size=64):\n",
    "        # Storing the height, width and putting down the channel\n",
    "        img_h, img_w, _ = imgarray.shape\n",
    "        \n",
    "        #Check if the face has area\n",
    "        if section is None:\n",
    "            section = [0, 0, img_w, img_h]   #Put empty co-ordinates to section if section is none\n",
    "        \n",
    "        # Assign the follwing variables \n",
    "        (x, y, w, h) = section\n",
    "        \n",
    "        #Calculate the margin\n",
    "        margin = int(min(w,h) * margin / 100)\n",
    "        \n",
    "        x_a = x - margin\n",
    "        y_a = y - margin\n",
    "        x_b = x + w + margin\n",
    "        y_b = y + h + margin\n",
    "        \n",
    "        if x_a < 0:\n",
    "            x_b = min(x_b - x_a, img_w-1)\n",
    "            x_a = 0\n",
    "        if y_a < 0:\n",
    "            y_b = min(y_b - y_a, img_h-1)\n",
    "            y_a = 0\n",
    "        if x_b > img_w:\n",
    "            x_a = max(x_a - (x_b - img_w), 0)\n",
    "            x_b = img_w\n",
    "        if y_b > img_h:\n",
    "            y_a = max(y_a - (y_b - img_h), 0)\n",
    "            y_b = img_h\n",
    "         \n",
    "        # Cropping the face out of the whole image\n",
    "        cropped = imgarray[y_a: y_b, x_a: x_b]\n",
    "        \n",
    "        # Resize the image to the 3 x 3\n",
    "        resized_img = cv2.resize(cropped, (size, size), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Converting the resized image to a numpy array\n",
    "        resized_img = np.array(resized_img)\n",
    "        \n",
    "        # Returning the resized image with shape (size x size x 3) \n",
    "        return resized_img, (x_a, y_a, x_b - x_a, y_b - y_a)\n",
    "\n",
    "    def find_face (self):\n",
    "        \n",
    "        # Get a Face classifier \n",
    "        face_cascade = cv2.CascadeClassifier(self.haar_cascade_classifier)\n",
    "        \n",
    "        # Open WebCam\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        \n",
    "        while True:\n",
    "            if not video_capture.isOpened():\n",
    "                sleep(5)\n",
    "            \n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = video_capture.read()\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.2,\n",
    "                minNeighbors=10,\n",
    "                minSize=(self.face_size, self.face_size)\n",
    "            )\n",
    "            \n",
    "            # placeholder for cropped faces\n",
    "            face_imgs = np.empty ((len(faces), self.face_size, self.face_size, 3))\n",
    "            \n",
    "            # returns i as an integer and face as a seq[i]\n",
    "            for i, face in enumerate(faces):\n",
    "                face_img, cropped = self.crop_face (frame, face, margin=40, size=self.face_size)\n",
    "                # Taking the parameters of the cropped face\n",
    "                (x, y, w, h) = cropped\n",
    "                # Drawing a rectangle around face\n",
    "                cv2.rectangle (frame, (x, y), (x + w, y + h), (53,196,55), 2) \n",
    "                # Converting it into a format that can be accepted by predict ()\n",
    "                face_imgs [i,:,:,:] = face_img\n",
    "                    \n",
    "            if len(face_imgs) > 0:\n",
    "                # predict ages and genders of the detected faces\n",
    "                results = self.model.predict (face_imgs) # Putting the placholder for cropped faces \n",
    "                predicted_genders = results[0]  \n",
    "                ages = np.arange(0, 101).reshape(101, 1)\n",
    "                predicted_ages = results[1].dot(ages).flatten()\n",
    "                \n",
    "            # draw results\n",
    "            for i, face in enumerate(faces):\n",
    "                label = \"{}, {}\".format(int(predicted_ages[i]),\n",
    "                                        \"Female\" if predicted_genders[i][0] > 0.5 else \"Male\")\n",
    "                self.draw_label(frame, (face[0], face[1]), label)\n",
    "                \n",
    "            cv2.imshow('Predicted Age And Gender', frame)\n",
    "            if cv2.waitKey(5) == 27:  # ESC key press\n",
    "                break\n",
    "                \n",
    "        # When everything is done, release the capture\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    face = Face_Object(depth=16, width=8)\n",
    "\n",
    "    face.find_face ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
